{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8bbaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3100ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/flipkart_com-ecommerce_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87666fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Define stop words list\n",
    "stop_words = set([\n",
    "    'the', 'is', 'in', 'and', 'to', 'a', 'with', 'of', 'for', 'on', 'as', 'by', 'at', 'from', 'or', 'an'\n",
    "    # Add more common stop words as needed\n",
    "])\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text, stop_words = stop_words):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Convert to lower case\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stop words\n",
    "    else:\n",
    "        text = ''  # Handle non-string values\n",
    "    return text\n",
    "\n",
    "# Preprocess category function\n",
    "def preprocess_category(category_text, stop_words=stop_words):\n",
    "    return preprocess_text(category_text, stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a80b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocess text data (assuming preprocess_text and preprocess_category functions exist)\n",
    "data['processed_description'] = data['description'].apply(preprocess_text)\n",
    "data['processed_category'] = data['product_category_tree'].apply(preprocess_category)\n",
    "\n",
    "# Combine description and category into a single text field\n",
    "data['combined_text'] = data['processed_description'] + ' ' + data['processed_category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b411245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the combined text\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79204b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query, vectorizer):\n",
    "    query = preprocess_text(query, stop_words)\n",
    "    return vectorizer.transform([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f09d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search_ml(query, X, vectorizer):\n",
    "    query_vector = vectorize_query(query, vectorizer)\n",
    "    similarities = cosine_similarity(query_vector, X).flatten()\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28721cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_results(similarities, top_n=10):\n",
    "    ranked_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return ranked_indices\n",
    "\n",
    "# Retrieve and display product details\n",
    "def display_top_results(indices, data, top_n=10):\n",
    "    results_df = data.loc[indices[:top_n]]\n",
    "    results_df.reset_index(drop=True, inplace=True)  # Reset index to sequential\n",
    "    \n",
    "    return results_df[['product_name', 'product_url', 'retail_price', 'discounted_price']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8325841a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_url</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hariom enterprises Stainless Steel Knife</td>\n",
       "      <td>http://www.flipkart.com/hariom-enterprises-sta...</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tia by Ten on Ten Fashion Women's Push-up Bra</td>\n",
       "      <td>http://www.flipkart.com/tia-ten-fashion-women-...</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Imported Disposable Waxing Strips With Spatula...</td>\n",
       "      <td>http://www.flipkart.com/imported-disposable-wa...</td>\n",
       "      <td>599.0</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tirupati Knife Rocker Pizza Cutter</td>\n",
       "      <td>http://www.flipkart.com/tirupati-knife-rocker-...</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pedrini Steel Waiters Corkscrew</td>\n",
       "      <td>http://www.flipkart.com/pedrini-steel-waiters-...</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PeepalComm Multi Purpose 11 Function Multi Uti...</td>\n",
       "      <td>http://www.flipkart.com/peepalcomm-multi-purpo...</td>\n",
       "      <td>599.0</td>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stealodeal Grand Harvest 11 Function Multi Uti...</td>\n",
       "      <td>http://www.flipkart.com/stealodeal-grand-harve...</td>\n",
       "      <td>499.0</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Checkered Chef Steel All-Purpose Scissor</td>\n",
       "      <td>http://www.flipkart.com/checkered-chef-steel-a...</td>\n",
       "      <td>2260.0</td>\n",
       "      <td>1921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>www.thepaper.asia Floral Flower Art Canvas Pen...</td>\n",
       "      <td>http://www.flipkart.com/www-thepaper-asia-flor...</td>\n",
       "      <td>375.0</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ETTI Glass Necklace</td>\n",
       "      <td>http://www.flipkart.com/etti-glass-necklace/p/...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name  \\\n",
       "0           Hariom enterprises Stainless Steel Knife   \n",
       "1      Tia by Ten on Ten Fashion Women's Push-up Bra   \n",
       "2  Imported Disposable Waxing Strips With Spatula...   \n",
       "3                 Tirupati Knife Rocker Pizza Cutter   \n",
       "4                    Pedrini Steel Waiters Corkscrew   \n",
       "5  PeepalComm Multi Purpose 11 Function Multi Uti...   \n",
       "6  Stealodeal Grand Harvest 11 Function Multi Uti...   \n",
       "7           Checkered Chef Steel All-Purpose Scissor   \n",
       "8  www.thepaper.asia Floral Flower Art Canvas Pen...   \n",
       "9                                ETTI Glass Necklace   \n",
       "\n",
       "                                         product_url  retail_price  \\\n",
       "0  http://www.flipkart.com/hariom-enterprises-sta...        1499.0   \n",
       "1  http://www.flipkart.com/tia-ten-fashion-women-...        1199.0   \n",
       "2  http://www.flipkart.com/imported-disposable-wa...         599.0   \n",
       "3  http://www.flipkart.com/tirupati-knife-rocker-...        1018.0   \n",
       "4  http://www.flipkart.com/pedrini-steel-waiters-...         300.0   \n",
       "5  http://www.flipkart.com/peepalcomm-multi-purpo...         599.0   \n",
       "6  http://www.flipkart.com/stealodeal-grand-harve...         499.0   \n",
       "7  http://www.flipkart.com/checkered-chef-steel-a...        2260.0   \n",
       "8  http://www.flipkart.com/www-thepaper-asia-flor...         375.0   \n",
       "9  http://www.flipkart.com/etti-glass-necklace/p/...        1000.0   \n",
       "\n",
       "   discounted_price  \n",
       "0             450.0  \n",
       "1             270.0  \n",
       "2             499.0  \n",
       "3             649.0  \n",
       "4             300.0  \n",
       "5             349.0  \n",
       "6             299.0  \n",
       "7            1921.0  \n",
       "8             280.0  \n",
       "9             199.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example search query\n",
    "query = \"Knife\"\n",
    "similarities = search_ml(query, X, vectorizer)\n",
    "top_indices = get_top_results(similarities, top_n=10)\n",
    "\n",
    "# Retrieve and display product details\n",
    "top_results = display_top_results(top_indices, data, top_n=10)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa62100",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f52c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example of fine-tuning parameters\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.85,  # Ignore terms that appear in more than 85% of documents\n",
    "    min_df=2,     # Include terms that appear in at least 2 documents\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    max_features=5000    # Limit to the top 5000 features\n",
    ")\n",
    "X = vectorizer.fit_transform(data['combined_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8039c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train your own Word2Vec model\n",
    "sentences = [desc.split() for desc in data['processed_description']]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ba8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_text(text, model):\n",
    "    words = text.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Vectorize documents\n",
    "doc_vectors = np.array([vectorize_text(desc, model) for desc in data['processed_description']])\n",
    "\n",
    "# Vectorize query\n",
    "def vectorize_query(query, model):\n",
    "    return vectorize_text(preprocess_text(query, stop_words), model)\n",
    "\n",
    "# Compute similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search_word2vec(query, doc_vectors, model):\n",
    "    query_vector = vectorize_query(query, model)\n",
    "    similarities = cosine_similarity([query_vector], doc_vectors).flatten()\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfcd00b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/glove.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18260\\4111074539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mglove_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_glove_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'path/to/glove.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectorize_text_glove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18260\\4111074539.py\u001b[0m in \u001b[0;36mload_glove_model\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_glove_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mglove_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/glove.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_model(file_path):\n",
    "    glove_model = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_model[word] = vector\n",
    "    return glove_model\n",
    "\n",
    "glove_model = load_glove_model('path/to/glove.txt')\n",
    "\n",
    "def vectorize_text_glove(text, glove_model):\n",
    "    words = text.split()\n",
    "    vectors = [glove_model.get(word, np.zeros(50)) for word in words]  # assuming 50 dimensions\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(50)\n",
    "\n",
    "# Vectorize documents\n",
    "doc_vectors_glove = np.array([vectorize_text_glove(desc, glove_model) for desc in data['processed_description']])\n",
    "\n",
    "# Vectorize query\n",
    "def vectorize_query_glove(query, glove_model):\n",
    "    return vectorize_text_glove(preprocess_text(query, stop_words), glove_model)\n",
    "\n",
    "# Compute similarity\n",
    "def search_glove(query, doc_vectors_glove, glove_model):\n",
    "    query_vector = vectorize_query_glove(query, glove_model)\n",
    "    similarities = cosine_similarity([query_vector], doc_vectors_glove).flatten()\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93813428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Load your preprocessed text data\n",
    "data['processed_description'] = data['description'].apply(preprocess_text)\n",
    "\n",
    "# Prepare sentences for training Word2Vec model\n",
    "sentences = data['processed_description'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc123bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, model):\n",
    "    \"\"\"\n",
    "    Convert text to a vector using Word2Vec embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: A string containing the processed text.\n",
    "    - model: Trained Word2Vec model.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array representing the average vector of the words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Vectorize all product descriptions\n",
    "doc_vectors = np.array([vectorize_text(desc, model) for desc in data['processed_description']])\n",
    "\n",
    "def vectorize_query(query, model, stop_words):\n",
    "    \"\"\"\n",
    "    Vectorize the search query after preprocessing it.\n",
    "    \n",
    "    Parameters:\n",
    "    - query: Search query string.\n",
    "    - model: Trained Word2Vec model.\n",
    "    - stop_words: List of stop words to exclude.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array representing the average vector of the words in the query.\n",
    "    \"\"\"\n",
    "    processed_query = preprocess_text(query, stop_words)\n",
    "    return vectorize_text(processed_query, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "stop_words = set(['for', 'the', 'and', 'to', 'in', 'on', 'with', 'a', 'of', 'is', 'it', 'by', 'at', 'as', 'an', 'this', 'that'])\n",
    "\n",
    "# Sample query\n",
    "query = \"shampoo for dog\"\n",
    "\n",
    "# Search for relevant products\n",
    "ranked_indices, similarity_scores = search_word2vec(query, doc_vectors, model, data, stop_words)\n",
    "\n",
    "# Retrieve top 10 product details\n",
    "def display_top_results(ranked_indices, similarity_scores, data, top_n=10):\n",
    "    \"\"\"\n",
    "    Display the top search results for a given query.\n",
    "    \n",
    "    Parameters:\n",
    "    - ranked_indices: Indices of ranked product descriptions.\n",
    "    - similarity_scores: Corresponding similarity scores.\n",
    "    - data: DataFrame containing product details.\n",
    "    - top_n: Number of top results to display.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing top N products sorted by similarity score.\n",
    "    \"\"\"\n",
    "    results_df = data.iloc[ranked_indices[:top_n]]\n",
    "    results_df['similarity_score'] = similarity_scores[:top_n]\n",
    "    return results_df[['product_name', 'product_category', 'product_url', 'retail_price', 'discounted_price', 'similarity_score']]\n",
    "\n",
    "# Display the top results\n",
    "top_results = display_top_results(ranked_indices, similarity_scores, data, top_n=10)\n",
    "top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c071dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text, stop_words):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    else:\n",
    "        text = ''  # Handle non-string values\n",
    "    return text\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(['for', 'the', 'and', 'to', 'in', 'on', 'with', 'a', 'of', 'is', 'it', 'by', 'at', 'as', 'an', 'this', 'that'])\n",
    "\n",
    "# Apply preprocessing to both descriptions and categories\n",
    "data['processed_description'] = data['description'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "data['processed_category'] = data['product_category_tree'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "\n",
    "# Prepare sentences for training Word2Vec model\n",
    "sentences = data['processed_description'].apply(lambda x: x.split()).tolist()\n",
    "sentences += data['processed_category'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84ff3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c847b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e46b3fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18260\\1803354060.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Vectorize product names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mproduct_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mml_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             raise ValueError(\n\u001b[1;32m--> 227\u001b[1;33m                 \u001b[1;34m\"np.nan is an invalid document, expected byte or unicode string.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m             )\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vectorize product names\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "product_vectors = vectorizer.fit_transform(data['product_name'])\n",
    "\n",
    "def ml_search(search_term, data):\n",
    "    # Vectorize search term\n",
    "    search_vector = vectorizer.transform([search_term])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(search_vector, product_vectors).flatten()\n",
    "    \n",
    "    # Get top matching products\n",
    "    top_indices = similarities.argsort()[-10:][::-1]\n",
    "    results = data.iloc[top_indices]\n",
    "    \n",
    "    return results[['product_name', 'brand', 'retail_price', 'overall_rating']]\n",
    "\n",
    "# Example search\n",
    "search_term = \"wireless earphones\"\n",
    "results = ml_search(search_term, data)\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f4f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c6719e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " uniq_id                       2\n",
      "crawl_timestamp               2\n",
      "product_url                   2\n",
      "product_name                  2\n",
      "product_category_tree         2\n",
      "pid                           2\n",
      "retail_price                 80\n",
      "discounted_price             80\n",
      "image                         5\n",
      "is_FK_Advantage_product       2\n",
      "description                   4\n",
      "product_rating                2\n",
      "overall_rating                2\n",
      "brand                      5866\n",
      "product_specifications       16\n",
      "processed_description         0\n",
      "processed_category            0\n",
      "combined_text                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "could not convert string to float: 'No rating available'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mnanmedian\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"f8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'No rating available'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18260\\2515322146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Fill or remove missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product_rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'overall_rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'overall_rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10800\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10801\u001b[0m         ):\n\u001b[1;32m> 10802\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10804\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"median\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmedian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10374\u001b[0m         return self._stat_function(\n\u001b[1;32m> 10375\u001b[1;33m             \u001b[1;34m\"median\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanmedian\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10376\u001b[0m         )\n\u001b[0;32m  10377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10353\u001b[0m             )\n\u001b[0;32m  10354\u001b[0m         return self._reduce(\n\u001b[1;32m> 10355\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10356\u001b[0m         )\n\u001b[0;32m  10357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4390\u001b[0m                 )\n\u001b[0;32m   4391\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4392\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4394\u001b[0m     def _reindex_indexer(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    154\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mnanmedian\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m             \u001b[1;31m# e.g. \"could not convert string to float: 'a'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: could not convert string to float: 'No rating available'"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Fill or remove missing values\n",
    "data['product_rating'].fillna(data['product_rating'].median(), inplace=True)\n",
    "data['overall_rating'].fillna(data['overall_rating'].median(), inplace=True)\n",
    "data['description'].fillna('', inplace=True)\n",
    "data['product_specifications'].fillna('', inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert data types if necessary\n",
    "data['retail_price'] = data['retail_price'].astype(float)\n",
    "data['discounted_price'] = data['discounted_price'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c10746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fill missing values in text fields with empty strings\n",
    "text_columns = ['product_name', 'description', 'product_category_tree', 'brand']\n",
    "data[text_columns] = data[text_columns].fillna('')\n",
    "\n",
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Clean text fields\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "data['product_name'] = data['product_name'].apply(clean_text)\n",
    "data['description'] = data['description'].apply(clean_text)\n",
    "data['product_category_tree'] = data['product_category_tree'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e53600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (20001, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine relevant text fields into a single column for vectorization\n",
    "data['text'] = data['product_name'] + ' ' + data['description'] + ' ' + data['product_category_tree']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Display the shape of the TF-IDF matrix\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2260100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vectors shape: (20001, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['tokens'] = data['text'].apply(word_tokenize)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get Word2Vec vectors for a product\n",
    "def get_word2vec_vector(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        # Handle case where no tokens are in the vocabulary\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    vector = np.mean(vectors, axis=0)\n",
    "    return vector\n",
    "\n",
    "# Apply the function to get Word2Vec vectors\n",
    "word2vec_vectors = np.array([get_word2vec_vector(text) for text in data['text']])\n",
    "\n",
    "# Display the shape of the Word2Vec vectors\n",
    "print(\"Word2Vec vectors shape:\", word2vec_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4567af1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>product_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>himmlisch stmpl1525 car armrestbeige car armrest</td>\n",
       "      <td>HIMMLISCH</td>\n",
       "      <td>5999.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/himmlisch-stmpl1525-ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>accessoreez crtrtbl052 cup holder car tray table</td>\n",
       "      <td>ACCESSOREEZ</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/accessoreez-crtrtbl052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>vetra e00ty19 multifunction car tray for marut...</td>\n",
       "      <td>Vetra</td>\n",
       "      <td>799.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/vetra-e00ty19-multifun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8094</th>\n",
       "      <td>vetra e00ty112 multifunction car tray for maru...</td>\n",
       "      <td>Vetra</td>\n",
       "      <td>799.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/vetra-e00ty112-multifu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>vetra e00ty09 multifunction car tray for marut...</td>\n",
       "      <td>Vetra</td>\n",
       "      <td>799.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/vetra-e00ty09-multifun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8114</th>\n",
       "      <td>vetra e00ty43 multifunction car tray for marut...</td>\n",
       "      <td>Vetra</td>\n",
       "      <td>799.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/vetra-e00ty43-multifun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104</th>\n",
       "      <td>vetra e00ty27 multifunction car tray for hyund...</td>\n",
       "      <td>Vetra</td>\n",
       "      <td>799.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/vetra-e00ty27-multifun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8116</th>\n",
       "      <td>vetra e00ty104 multifunction car tray for tata...</td>\n",
       "      <td>Vetra</td>\n",
       "      <td>799.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/vetra-e00ty104-multifu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8098</th>\n",
       "      <td>vetra e00ty71 multifunction car tray for hyund...</td>\n",
       "      <td>Vetra</td>\n",
       "      <td>799.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/vetra-e00ty71-multifun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>packy poda car mat maruti alto</td>\n",
       "      <td>Packy Poda</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>http://www.flipkart.com/packy-poda-car-mat-mar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            product_name        brand  \\\n",
       "10286   himmlisch stmpl1525 car armrestbeige car armrest    HIMMLISCH   \n",
       "1510    accessoreez crtrtbl052 cup holder car tray table  ACCESSOREEZ   \n",
       "8113   vetra e00ty19 multifunction car tray for marut...        Vetra   \n",
       "8094   vetra e00ty112 multifunction car tray for maru...        Vetra   \n",
       "8123   vetra e00ty09 multifunction car tray for marut...        Vetra   \n",
       "8114   vetra e00ty43 multifunction car tray for marut...        Vetra   \n",
       "8104   vetra e00ty27 multifunction car tray for hyund...        Vetra   \n",
       "8116   vetra e00ty104 multifunction car tray for tata...        Vetra   \n",
       "8098   vetra e00ty71 multifunction car tray for hyund...        Vetra   \n",
       "7515                      packy poda car mat maruti alto   Packy Poda   \n",
       "\n",
       "       retail_price       overall_rating  \\\n",
       "10286        5999.0  No rating available   \n",
       "1510         1200.0  No rating available   \n",
       "8113          799.0  No rating available   \n",
       "8094          799.0  No rating available   \n",
       "8123          799.0  No rating available   \n",
       "8114          799.0  No rating available   \n",
       "8104          799.0  No rating available   \n",
       "8116          799.0  No rating available   \n",
       "8098          799.0  No rating available   \n",
       "7515         2000.0  No rating available   \n",
       "\n",
       "                                             product_url  \n",
       "10286  http://www.flipkart.com/himmlisch-stmpl1525-ca...  \n",
       "1510   http://www.flipkart.com/accessoreez-crtrtbl052...  \n",
       "8113   http://www.flipkart.com/vetra-e00ty19-multifun...  \n",
       "8094   http://www.flipkart.com/vetra-e00ty112-multifu...  \n",
       "8123   http://www.flipkart.com/vetra-e00ty09-multifun...  \n",
       "8114   http://www.flipkart.com/vetra-e00ty43-multifun...  \n",
       "8104   http://www.flipkart.com/vetra-e00ty27-multifun...  \n",
       "8116   http://www.flipkart.com/vetra-e00ty104-multifu...  \n",
       "8098   http://www.flipkart.com/vetra-e00ty71-multifun...  \n",
       "7515   http://www.flipkart.com/packy-poda-car-mat-mar...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_word2vec_vector(text, model):\n",
    "    tokens = word_tokenize(text)\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        # Handle case where no tokens are in the vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "    vector = np.mean(vectors, axis=0)\n",
    "    return vector\n",
    "\n",
    "def search_products_word2vec(query, embeddings, model, data, top_n=10):\n",
    "    # Convert the query to a vector using the Word2Vec model\n",
    "    query_embedding = get_word2vec_vector(query, model).reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarities between the query and product embeddings\n",
    "    similarities = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar products\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "    # Return the top N most similar products\n",
    "    return data.iloc[top_indices][['product_name', 'brand', 'retail_price', 'overall_rating', 'product_url']]\n",
    "\n",
    "# Example usage\n",
    "search_term = \"car\"\n",
    "results_word2vec = search_products_word2vec(search_term, word2vec_vectors, word2vec_model, data)\n",
    "results_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfab6b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18260\\3620705393.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Should print True if CUDA is available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Prints the current device index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Prints the name of the current device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcabf8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1891f5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14012\\2656845116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbert_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_bert_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2894\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[0mis_quantized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_in_8bit\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2896\u001b[1;33m                 \u001b[0mkeep_in_fp32_modules\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_in_fp32_modules\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2897\u001b[0m             )\n\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3180\u001b[0m                 \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3181\u001b[0m             )\n\u001b[1;32m-> 3182\u001b[1;33m             \u001b[0merror_msgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_state_dict_into_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3183\u001b[0m             \u001b[0moffload_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_model\u001b[1;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[0;32m    548\u001b[0m                 \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m     \u001b[1;31m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;31m# it's safe to delete it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                 \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                 \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                 \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                 \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                 \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    542\u001b[0m                             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_from_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m                 \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_from_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m   1570\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1572\u001b[1;33m                         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1573\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1574\u001b[0m                     error_msgs.append('While copying the parameter named \"{}\", '\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, model, tokenizer, batch_size=64):\n",
    "    embeddings = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_texts = texts[start:end]\n",
    "        # Tokenize texts and move tensors to GPU if available\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Take the mean of the token embeddings (CLS token can be used too)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)  # Stack arrays vertically\n",
    "\n",
    "# Example usage\n",
    "texts = [\"This is a sample sentence.\", \"Here is another one.\"]\n",
    "bert_embeddings = get_bert_embeddings(texts, bert_model, tokenizer)\n",
    "print(bert_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6368b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14012\\303095876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Should print True if CUDA is available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Prints the current device index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Prints the name of the current device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available\n",
    "print(torch.cuda.current_device())  # Prints the current device index\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))  # Prints the name of the current device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27728387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847aee7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Retrieval sys",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
